{"cells":[{"cell_type":"markdown","metadata":{"id":"VqEpGyyyGE1Z","tags":["pdf-title"]},"source":["## Задача №3\n","\n","В данном задании вам необходимо реализовать функции ошибки для линейной регрессии и их производные по параметрам, __не используя автоматические дифференцирование.__ Все методы должны быть реализованы только с использованием библиотеки `numpy`.\n","\n","Ваша основная задача: вывести формулы для производных __MSE, MAE, L1 и L2 регуляризационных членов__ в _векторном случае_ (т.е. когда и объект $\\mathbf{x}_i$, и целевое значение $\\mathbf{y}_i$ являются векторами.\n","\n","\n","Для работы обратимся к [Boston housing prices dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html). Он был предобработан для вашего удобства и будет загружен ниже."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"MnG-xnlGEVST","executionInfo":{"status":"ok","timestamp":1737943546109,"user_tz":-180,"elapsed":988,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}},"outputId":"b2f2d0f2-6822-4773-a1a2-e60083f730d8","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-01-27 02:05:44--  https://raw.githubusercontent.com/girafe-ai/ml-course/23f_yandex_ml_trainings/homeworks/assignment03_derivatives/boston_subset.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17921 (18K) [text/plain]\n","Saving to: ‘boston_subset.json.1’\n","\n","boston_subset.json. 100%[===================>]  17.50K  --.-KB/s    in 0.001s  \n","\n","2025-01-27 02:05:45 (29.6 MB/s) - ‘boston_subset.json.1’ saved [17921/17921]\n","\n"]}],"source":["\"\"\"\n","If you are using Google Colab, uncomment the next line to download `boston_subset.json`\n","\"\"\"\n","!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23f_yandex_ml_trainings/homeworks/assignment03_derivatives/boston_subset.json"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"8lQUR89nGE1f","executionInfo":{"status":"ok","timestamp":1737943546998,"user_tz":-180,"elapsed":11,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["# Run some setup code for this notebook.\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"OGf3ShTNGE1q","executionInfo":{"status":"ok","timestamp":1737943546998,"user_tz":-180,"elapsed":11,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["import json\n","\n","with open(\"boston_subset.json\", \"r\") as iofile:\n","    dataset = json.load(iofile)\n","feature_matrix = np.array(dataset[\"data\"])\n","targets = np.array(dataset[\"target\"])\n"]},{"cell_type":"markdown","metadata":{"id":"WbBc_5FhGE2B"},"source":["## Имплементация функций потерь и методов регуляризации.\n","Для того, чтобы решить задание, вам необходимо реализовать все методы в файле `loss_and_derivatives.py`. Для вашего удобства код скопирован внутрь ноутбука в следующую ячейку. После решения ноутбука можете просто перенести его в .py файл.\n","__Внимание, в данном задании не требуется использовать свободный член (bias term)__, т.е. линейная модель примет простой вид\n","$$\n","\\hat{\\mathbf{y}} = XW\n","$$\n","Единичный столбец также не добавляется к матрице $X$.\n","\n","Реализуйте методы для MSE, MAE, L1 и L2 регуляризации, а также вычисления их производных (опциональное задание) по параметрам линейной модели.\n","\n","__Для вашего удобства данные уже предобработаны, и использование линейной модели без свободного члена не является ошибкой. В данном задании он не должен быть использован.__"]},{"cell_type":"code","source":["len(targets.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2L2SJwxVIje","executionInfo":{"status":"ok","timestamp":1737943546998,"user_tz":-180,"elapsed":10,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}},"outputId":"dc2c936e-d173-46e5-fbad-2042d41a6ae3"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"dtELlRTOGE2E","tags":["pdf-ignore"],"executionInfo":{"status":"ok","timestamp":1737943546998,"user_tz":-180,"elapsed":9,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["import numpy as np\n","\n","\n","class LossAndDerivatives:\n","    @staticmethod\n","    def mse(X, Y, w):\n","        \"\"\"\n","        X : numpy array of shape (`n_observations`, `n_features`)\n","        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n","        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n","\n","        Return : float\n","            single number with MSE value of linear model (X.dot(w)) with no bias term\n","            on the selected dataset.\n","\n","        Comment: If Y is two-dimentional, average the error over both dimentions.\n","        \"\"\"\n","        ans=0\n","        for i in range(len(Y.shape)):\n","          ans = ans + np.mean((X.dot(w) - Y[:][i]) ** 2)\n","        ans=ans/len(Y.shape)\n","        return ans\n","\n","    @staticmethod\n","    def mae(X, Y, w):\n","        \"\"\"\n","        X : numpy array of shape (`n_observations`, `n_features`)\n","        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n","        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n","\n","        Return: float\n","            single number with MAE value of linear model (X.dot(w)) with no bias term\n","            on the selected dataset.\n","\n","        Comment: If Y is two-dimentional, average the error over both dimentions.\n","        \"\"\"\n","\n","        # YOUR CODE HERE\n","        ans=0\n","        for i in range(len(Y.shape)):\n","          ans = ans + np.mean(abs(X.dot(w)-Y[:][i]))\n","        ans=ans/len(Y.shape)\n","        return ans\n","\n","\n","    @staticmethod\n","    def l2_reg(w):\n","        \"\"\"\n","        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n","\n","        Return: float\n","            single number with sum of squared elements of the weight matrix ( \\sum_{ij} w_{ij}^2 )\n","\n","        Computes the L2 regularization term for the weight matrix w.\n","        \"\"\"\n","\n","        # YOUR CODE HERE\n","        return np.sum(w**2)\n","\n","    @staticmethod\n","    def l1_reg(w):\n","        \"\"\"\n","        w : numpy array of shape (`n_features`, `target_dimentionality`)\n","\n","        Return : float\n","            single number with sum of the absolute values of the weight matrix ( \\sum_{ij} |w_{ij}| )\n","\n","        Computes the L1 regularization term for the weight matrix w.\n","        \"\"\"\n","\n","        # YOUR CODE HERE\n","        return np.sum(abs(w))\n","\n","    @staticmethod\n","    def no_reg(w):\n","        \"\"\"\n","        Simply ignores the regularization\n","        \"\"\"\n","        return 0.0\n","\n","    @staticmethod\n","    def mse_derivative(X, Y, w):\n","        \"\"\"\n","        X : numpy array of shape (`n_observations`, `n_features`)\n","        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n","        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n","\n","        Return : numpy array of same shape as `w`\n","\n","        Computes the MSE derivative for linear regression (X.dot(w)) with no bias term\n","        w.r.t. w weight matrix.\n","\n","        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n","        dimension as well, so you need to consider that fact in derivative implementation.\n","        \"\"\"\n","        mult=list(map(lambda Y: 1 if len(Y.shape)==1 else len(Y.shape[1]), Y))[0]\n","        ans=0\n","        for i in range(mult):\n","          derivative=X.T.dot(X.dot(w)-Y[:][i])/Y.shape[0]\n","          ans = ans + derivative\n","        ans=ans/len(mult)\n","        return ans\n","\n","    @staticmethod\n","    def mae_derivative(X, Y, w):\n","        \"\"\"\n","        X : numpy array of shape (`n_observations`, `n_features`)\n","        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n","        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n","\n","        Return : numpy array of same shape as `w`\n","\n","        Computes the MAE derivative for linear regression (X.dot(w)) with no bias term\n","        w.r.t. w weight matrix.\n","\n","        Please mention, that in case `target_dimentionality` > 1 the error is averaged along this\n","        dimension as well, so you need to consider that fact in derivative implementation.\n","        \"\"\"\n","\n","        # YOUR CODE HERE\n","        mult=list(map(lambda Y: 1 if len(Y.shape)==1 else len(Y.shape[1]), Y))[0]\n","        ans=np.zeros_like(w)\n","        for i in range(mult):\n","          for k in range(len(w)):\n","            ans[k]=lambda diff: 0 if diff==0 else 1 if diff>0 else -1, list(X.dot(w)-Y[:][i])[k]\n","            ans2=ans2+ans\n","\n","        ans2=ans2/mult\n","        return ans2\n","\n","\n","    @staticmethod\n","    def l2_reg_derivative(w):\n","        \"\"\"\n","        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n","\n","        Return : numpy array of same shape as `w`\n","\n","        Computes the L2 regularization term derivative w.r.t. the weight matrix w.\n","        \"\"\"\n","\n","        # YOUR CODE HERE\n","        return 2*w\n","\n","    @staticmethod\n","    def l1_reg_derivative(w):\n","        \"\"\"\n","        Y : numpy array of shape (`n_observations`, `target_dimentionality`) or (`n_observations`,)\n","        w : numpy array of shape (`n_features`, `target_dimentionality`) or (`n_features`,)\n","\n","        Return : numpy array of same shape as `w`\n","\n","        Computes the L1 regularization term derivative w.r.t. the weight matrix w.\n","        \"\"\"\n","\n","        # YOUR CODE HERE\n","        return np.array(list(map(lambda diff: 1 if diff>0 else 0 if diff==0 else -1, list(w))))\n","\n","    @staticmethod\n","    def no_reg_derivative(w):\n","        \"\"\"\n","        Simply ignores the derivative\n","        \"\"\"\n","        return np.zeros_like(w)"]},{"cell_type":"markdown","metadata":{"id":"kAkqemzMEVSV"},"source":["Обращаем ваше внимание, требуется реализовать решение в векторном виде (т.е. для каждого объекта предсказание $\\hat{\\mathbf{y}}$ является вектором с размерностью $\\geq 1$.\n","\n","__Внимание! При подсчете ошибки она усредняется как по объектам, так и по размерности y. Аналогичное верно и для производных__.\n","\n","Например, для вектора отклонений на одном объекте $[1., 1., 1., 1.]$ значение функции ошибки будет равно $\\frac{1}{4}(1. + 1. + 1. + 1.)$\n","\n","Для вашего удобства метод `.mse` уже реализован и вы можете обращаться к нему за примером."]},{"cell_type":"markdown","metadata":{"id":"sMN81aYyGE2T"},"source":["Для проверки своего кода вам доступно несколько assert'ов:"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"UtkO4hWYGE2c","executionInfo":{"status":"error","timestamp":1737943546999,"user_tz":-180,"elapsed":10,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}},"colab":{"base_uri":"https://localhost:8080/","height":211},"outputId":"cad2a1a2-e230-4431-a99d-2535d9983ba2"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'x_n' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-549423d6fe87>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0mreference_l2_reg_derivative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.54\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2.54\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_mse_derivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLossAndDerivatives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m assert np.allclose(\n\u001b[1;32m     10\u001b[0m     \u001b[0mreference_mse_derivative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLossAndDerivatives\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'x_n' is not defined"]}],"source":["reference_mse_derivative = np.array(\n","    [\n","        [7.32890068, 12.88731311, 18.82128365, 23.97731238],\n","        [9.55674399, 17.05397661, 24.98807528, 32.01723714],\n","    ]\n",")\n","reference_l2_reg_derivative = np.array([[2.54, 2.44, 2.9, 2.2], [2.54, 2.44, 2.9, 2.2]])\n","print(reference_mse_derivative, LossAndDerivatives.mse_derivative(x_n, y_n, w))\n","assert np.allclose(\n","    reference_mse_derivative, LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3\n","), \"Something wrong with MSE derivative\"\n","\n","assert np.allclose(\n","    reference_l2_reg_derivative, LossAndDerivatives.l2_reg_derivative(w), rtol=1e-3\n","), \"Something wrong with L2 reg derivative\"\n","\n","print(\n","    \"MSE derivative:\\n{} \\n\\nL2 reg derivative:\\n{}\".format(\n","        LossAndDerivatives.mse_derivative(x_n, y_n, w),\n","        LossAndDerivatives.l2_reg_derivative(w),\n","    )\n",")"]},{"cell_type":"code","source":["y_n"],"metadata":{"id":"roGOe5s6MBhZ","executionInfo":{"status":"aborted","timestamp":1737943546999,"user_tz":-180,"elapsed":8,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KP8CJE8nEVSW","executionInfo":{"status":"aborted","timestamp":1737943546999,"user_tz":-180,"elapsed":7,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["reference_mae_derivative = np.array(\n","    [\n","        [0.19708867, 0.19621798, 0.19621798, 0.19572906],\n","        [0.25574138, 0.25524507, 0.25524507, 0.25406404],\n","    ]\n",")\n","reference_l1_reg_derivative = np.array([[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]])\n","\n","print(reference_mae_derivative)\n","print(LossAndDerivatives.mae_derivative(x_n, y_n, w))\n","\n","assert np.allclose(\n","    reference_mae_derivative, LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3\n","), \"Something wrong with MAE derivative\"\n","\n","assert np.allclose(\n","    reference_l1_reg_derivative, LossAndDerivatives.l1_reg_derivative(w), rtol=1e-3\n","), \"Something wrong with L1 reg derivative\"\n","\n","print(\n","    \"MAE derivative:\\n{} \\n\\nL1 reg derivative:\\n{}\".format(\n","        LossAndDerivatives.mae_derivative(x_n, y_n, w),\n","        LossAndDerivatives.l1_reg_derivative(w),\n","    )\n",")"]},{"cell_type":"markdown","metadata":{"id":"kJcSPj8UGE20"},"source":["### Градиентный спуск для решения реальной задачи\n","Следующая функция позволяет найти оптимальные значения параметров с помощью градиентного спуска:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"On6aSWuIGE21","executionInfo":{"status":"aborted","timestamp":1737943546999,"user_tz":-180,"elapsed":7,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["def get_w_by_grad(\n","    X, Y, w_0, loss_mode=\"mse\", reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05\n","):\n","    if loss_mode == \"mse\":\n","        loss_function = LossAndDerivatives.mse\n","        loss_derivative = LossAndDerivatives.mse_derivative\n","    elif loss_mode == \"mae\":\n","        loss_function = LossAndDerivatives.mae\n","        loss_derivative = LossAndDerivatives.mae_derivative\n","    else:\n","        raise ValueError(\n","            \"Unknown loss function. Available loss functions: `mse`, `mae`\"\n","        )\n","\n","    if reg_mode is None:\n","        reg_function = LossAndDerivatives.no_reg\n","        reg_derivative = (\n","            LossAndDerivatives.no_reg_derivative\n","        )  # lambda w: np.zeros_like(w)\n","    elif reg_mode == \"l2\":\n","        reg_function = LossAndDerivatives.l2_reg\n","        reg_derivative = LossAndDerivatives.l2_reg_derivative\n","    elif reg_mode == \"l1\":\n","        reg_function = LossAndDerivatives.l1_reg\n","        reg_derivative = LossAndDerivatives.l1_reg_derivative\n","    else:\n","        raise ValueError(\n","            \"Unknown regularization mode. Available modes: `l1`, `l2`, None\"\n","        )\n","\n","    w = w_0.copy()\n","\n","    for i in range(n_steps):\n","        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)\n","        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)\n","        gradient_norm = np.linalg.norm(gradient)\n","        if gradient_norm > 5.0:\n","            gradient = gradient / gradient_norm * 5.0\n","        w -= lr * gradient\n","\n","        if i % 25 == 0:\n","            print(\n","                \"Step={}, loss={},\\ngradient values={}\\n\".format(\n","                    i, empirical_risk, gradient\n","                )\n","            )\n","    return w"]},{"cell_type":"markdown","metadata":{"id":"a1ytS7DnEVSX"},"source":["Рассмотрим простой пример:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1pyDIyqGE25","executionInfo":{"status":"aborted","timestamp":1737943546999,"user_tz":-180,"elapsed":7,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["# Initial weight matrix\n","w = np.ones((2, 1), dtype=float)\n","y_n = targets[:, None]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erTRQiAFGE29","executionInfo":{"status":"aborted","timestamp":1737943547000,"user_tz":-180,"elapsed":8,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["w_grad = get_w_by_grad(x_n, y_n, w, loss_mode=\"mse\", reg_mode=\"l2\", n_steps=250)"]},{"cell_type":"markdown","metadata":{"id":"Yqir6yaZEVSX"},"source":["### Сравнение с `sklearn`\n","Сравним реализованную модель с версией из `sklearn`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r8JtepFEVSY","executionInfo":{"status":"aborted","timestamp":1737943547000,"user_tz":-180,"elapsed":7,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["from sklearn.linear_model import Ridge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoLD0mYhEVSY","executionInfo":{"status":"aborted","timestamp":1737943547000,"user_tz":-180,"elapsed":7,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["lr = Ridge(alpha=0.05)\n","lr.fit(x_n, y_n)\n","print(\n","    \"sklearn linear regression implementation delivers MSE = {}\".format(\n","        np.mean((lr.predict(x_n) - y_n) ** 2)\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gse1m4nyGE3C","executionInfo":{"status":"aborted","timestamp":1737943547000,"user_tz":-180,"elapsed":7,"user":{"displayName":"Sergey Boguslavskiy","userId":"06001537057233291669"}}},"outputs":[],"source":["plt.scatter(x_n[:, -1], y_n[:, -1])\n","plt.scatter(\n","    x_n[:, -1],\n","    x_n.dot(w_grad)[:, -1],\n","    color=\"orange\",\n","    label=\"Handwritten linear regression\",\n","    linewidth=5,\n",")\n","plt.scatter(x_n[:, -1], lr.predict(x_n), color=\"cyan\", label=\"sklearn Ridge\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sVN0CI5ZEVSY"},"source":["Если в полученных решениях есть небольшие различия, это не страшно. Модель основанная на вашей реализации не использует свободный член (он равен $0$), в то время версия из `sklearn` настраивает и его."]},{"cell_type":"markdown","metadata":{"id":"6GgeWdBmGE3H"},"source":["### Сдача решения\n","Сдайте в чекер реализованный класс `LossAndDerivatives`. Для этого можете скопировать всю ячейку с кодом (в том числе и импортирование `numpy`) в файл `derivatives.py`.\n","\n","На этом задача завершена. Поздравляем!"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/girafe-ai/ml-course/blob/23f_yandex_ml_trainings/homeworks/assignment03_derivatives/derivatives_assignment_03.ipynb","timestamp":1734551268658}]},"kernelspec":{"display_name":"Py3 Research","language":"python","name":"py3_research_kernel"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}